{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header initialized\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math as math\n",
    "import janitor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import os\n",
    "\n",
    "exec(open(\"../header.py\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 40\n",
    "data_folder = processed_root(\"02-train-validation-test-split/threshold-\"+str(threshold)+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/processed/02-train-validation-test-split/threshold-30/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(data_folder+\"train_data.csv\")\n",
    "val_data = pd.read_csv(data_folder+\"val_data.csv\")\n",
    "test_data = pd.read_csv(data_folder+\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words_from_text(texts):\n",
    "    '''\n",
    "    Purpose: Helper function for bag_of_words\n",
    "    Input: texts\n",
    "    Output: list of words that occur in more than threshold texts\n",
    "    '''\n",
    "    \n",
    "    threshold = 5\n",
    "    word_counts = {}\n",
    "    \n",
    "    for text in texts:\n",
    "        for word in text:\n",
    "            if word in word_counts:\n",
    "                word_counts[word] += 1\n",
    "            else:\n",
    "                word_counts[word] = 1 \n",
    "                \n",
    "    filtered_word_counts = word_counts.copy()\n",
    "\n",
    "    for i in word_counts:\n",
    "        if filtered_word_counts[i] < threshold:\n",
    "            filtered_word_counts.pop(i)\n",
    "            \n",
    "    return list(filtered_word_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(data, text_column):\n",
    "    '''\n",
    "    Purpose: Helper function for bag_of_words\n",
    "    Input: Dataset\n",
    "    Output: array of email sets of words (sets don't allow duplicates)\n",
    "    '''\n",
    "    \n",
    "    return(data.apply(lambda x:set(x[text_column].split(' ')), axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       {wind\\nand, main\\n\\nnor, sportively\\nglanc, qu...\n",
       "1       {sun, weary\\nno, old, sound\\nwhil, merri, dark...\n",
       "2       {say, mine\\nmer, bitter\\na, stronger\\ni, fully...\n",
       "3       {cling, shambl, enough, nose\\nh, wide, man\\nsh...\n",
       "4       {felt, –\\nmen, heaven, aw, behind\\nhunt, shudd...\n",
       "                              ...                        \n",
       "1118    {“behold, drink, mouth, “thinkest, world glory...\n",
       "1119    {light\\nof, meet\\n\\nbut, thi, coy, sweet\\nus, ...\n",
       "1120    {commandment”\\n\\nand, astonish, marveling\\ngre...\n",
       "1121    {be\\ntray, true, relat, isn't, alon, pear, mig...\n",
       "1122    {, genit, true, youth, unreason, hill, visual,...\n",
       "Length: 1123, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_text(train_data, 'clean_content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(data, content_column, word_data = None):\n",
    "    '''\n",
    "    Purpose: Converts a dataset to bag of words format.\n",
    "    Input: Dataset\n",
    "    Output: Bag of words version of the data\n",
    "    '''\n",
    "    \n",
    "    texts = extract_text(data, content_column)\n",
    "        \n",
    "    if word_data is None:\n",
    "        bag = extract_words_from_text(texts)\n",
    "    else:\n",
    "        bag = extract_words_from_text(extract_text(word_data, content_column))\n",
    "    \n",
    "    word_occurence = words_in_texts(bag, texts)\n",
    "    \n",
    "    data = data.reset_index(drop = True)\n",
    "    \n",
    "    new_data = pd.DataFrame(data = word_occurence, columns = bag)\n",
    "    new_data.insert(0, 'poetry_text', data[content_column])\n",
    "    new_data['poetry_author'] = data['author']\n",
    "        \n",
    "    return(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_in_texts(words, texts):\n",
    "    '''\n",
    "    Args:\n",
    "        words (list-like): words to find\n",
    "        texts (Series): sets of words to search in\n",
    "    \n",
    "    Returns:\n",
    "        NumPy array of 0s and 1s with shape (n, p) where n is the\n",
    "        number of texts and p is the number of words.\n",
    "        \n",
    "        Only considers whole words, not partial.\n",
    "    '''\n",
    "    indicator_array = np.array([texts.map(lambda x:word in x) for word in words]).T\n",
    "    return indicator_array.astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run bag of words for each threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_datasets(df_dict, save_folder):\n",
    "    for i in df_dict:\n",
    "        try:\n",
    "            df_dict[i].to_csv(save_folder + \"/\" + i, index = False)\n",
    "        except FileNotFoundError:\n",
    "            os.mkdir(save_folder)\n",
    "            df_dict[i].to_csv(save_folder + \"/\" + i, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_for_threshold(threshold):\n",
    "    data_folder = processed_root(\"02-train-validation-test-split/threshold-\"+str(threshold)+\"/\")\n",
    "\n",
    "    train_data = pd.read_csv(data_folder+\"train_data.csv\")\n",
    "    val_data = pd.read_csv(data_folder+\"val_data.csv\")\n",
    "    test_data = pd.read_csv(data_folder+\"test_data.csv\")\n",
    "\n",
    "    bag_train_data = bag_of_words(train_data, content_column = 'clean_content')\n",
    "    bag_val_data = bag_of_words(val_data, \n",
    "                                content_column = 'clean_content', \n",
    "                                word_data = train_data)\n",
    "    bag_test_data = bag_of_words(test_data, \n",
    "                                 content_column = 'clean_content',\n",
    "                                 word_data = train_data)\n",
    "    print(\"Threshold:\", threshold)\n",
    "    print(\"Bag Train:\", bag_train_data.shape)\n",
    "    print(\"Bag Val:\", bag_val_data.shape)\n",
    "    print(\"Bag Test:\", bag_test_data.shape)\n",
    "    \n",
    "    dfs_to_save = {'bow_train_data.csv':bag_train_data,\n",
    "               'bow_val_data.csv':bag_val_data,\n",
    "               'bow_test_data.csv':bag_test_data}\n",
    "\n",
    "    save_datasets(dfs_to_save, save_folder = processed_root(\"03-bag-of-words/threshold-\"+str(threshold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 30\n",
      "Bag Train: (1123, 4192)\n",
      "Bag Val: (313, 4192)\n",
      "Bag Test: (140, 4192)\n",
      "Threshold: 40\n",
      "Bag Train: (457, 1899)\n",
      "Bag Val: (127, 1899)\n",
      "Bag Test: (59, 1899)\n",
      "Threshold: 50\n",
      "Bag Train: (241, 944)\n",
      "Bag Val: (69, 944)\n",
      "Bag Test: (31, 944)\n"
     ]
    }
   ],
   "source": [
    "bag_of_words_for_threshold(30)\n",
    "bag_of_words_for_threshold(40)\n",
    "bag_of_words_for_threshold(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
