{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_LutB7z7j56",
        "outputId": "52c91cbe-58f9-4880-f6b7-6b002bf8b167"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_CgGW5sGIi5"
      },
      "source": [
        "https://keras.io/examples/nlp/text_classification_with_transformer/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAhtIuWa_hf7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Dense, Embedding, LSTM, Bidirectional\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok89G0nc_wcz"
      },
      "source": [
        "pwd_train_40 = '/content/drive/MyDrive/Colab Notebooks/NLP proj/data_thresh40/train_data.csv'\n",
        "pwd_val_40 = '/content/drive/MyDrive/Colab Notebooks/NLP proj/data_thresh40/val_data.csv'\n",
        "pwd_test_40 = '/content/drive/MyDrive/Colab Notebooks/NLP proj/data_thresh40/test_data.csv'\n",
        "\n",
        "train_df = pd.read_csv(pwd_train_40)\n",
        "val_df = pd.read_csv(pwd_val_40)\n",
        "test_df = pd.read_csv(pwd_test_40)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0GzgxXi_2jm"
      },
      "source": [
        "X_train = train_df['clean_content']\n",
        "X_val = val_df['clean_content']\n",
        "X_test = test_df['clean_content']"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "totVI9p2_7Zp"
      },
      "source": [
        "vocab_size = 3000\n",
        "max_length = 300\n",
        "oov_token = \"<UNK>\"\n",
        "padding_type = \"post\"\n",
        "trunction_type='post'\n",
        "tokenizer = Tokenizer(num_words = vocab_size,oov_token=oov_token,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index = tokenizer.word_index\n"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6mxzT_-_9CH"
      },
      "source": [
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_train_padded = pad_sequences(X_train_sequences,maxlen=max_length, padding=padding_type, \n",
        "                       truncating=trunction_type)\n",
        "X_val_sequences = tokenizer.texts_to_sequences(X_val)\n",
        "X_val_padded = pad_sequences(X_val_sequences,maxlen=max_length, padding=padding_type, \n",
        "                       truncating=trunction_type)\n",
        "\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_padded = pad_sequences(X_test_sequences,maxlen=max_length, padding=padding_type, \n",
        "                       truncating=trunction_type)\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOShDYPm_9rD"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(train_df['author'])\n",
        "y_train=le.transform(train_df['author'])\n",
        "y_val=le.transform(val_df['author'])"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Zg06zDDAMXm"
      },
      "source": [
        "y_test=le.transform(test_df['author'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90ISLLaWATiK"
      },
      "source": [
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = layers.Dense(embed_dim)\n",
        "        self.key_dense = layers.Dense(embed_dim)\n",
        "        self.value_dense = layers.Dense(embed_dim)\n",
        "        self.combine_heads = layers.Dense(embed_dim)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        query = self.separate_heads(\n",
        "            query, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        key = self.separate_heads(\n",
        "            key, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        value = self.separate_heads(\n",
        "            value, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(\n",
        "            attention, perm=[0, 2, 1, 3]\n",
        "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
        "        concat_attention = tf.reshape(\n",
        "            attention, (batch_size, -1, self.embed_dim)\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        output = self.combine_heads(\n",
        "            concat_attention\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        return output\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F32xp6lpAX34"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-NQ5e_oAZnh"
      },
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYaWYuPECkn0",
        "outputId": "5d5a2b16-21d9-421a-d7ff-f93313edff0a"
      },
      "source": [
        "set(y_train)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMBoK4HYCy3C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFyVeQ7zAchg"
      },
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "nclasses = 12\n",
        "\n",
        "inputs = layers.Input(shape=(max_length,))\n",
        "embedding_layer = TokenAndPositionEmbedding(max_length, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(nclasses, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuw_N73-Ba6X"
      },
      "source": [
        "#lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "#    initial_learning_rate=1e-2,\n",
        "#    decay_steps=10000,\n",
        "#    decay_rate=0.9)\n",
        "#opt = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "#model.compile(opt, \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.compile('adam', \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9DQRrS9GWHQ",
        "outputId": "52dbdf19-dc61-41de-8794-77f01fae3d8b"
      },
      "source": [
        "history = model.fit(\n",
        "    X_train_padded, y_train, batch_size=32, epochs=40, validation_data=(X_val_padded, y_val)\n",
        ")"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "15/15 [==============================] - 3s 219ms/step - loss: 2.4888 - accuracy: 0.0919 - val_loss: 2.4416 - val_accuracy: 0.1024\n",
            "Epoch 2/40\n",
            "15/15 [==============================] - 3s 202ms/step - loss: 2.4342 - accuracy: 0.1050 - val_loss: 2.4209 - val_accuracy: 0.1024\n",
            "Epoch 3/40\n",
            "15/15 [==============================] - 3s 202ms/step - loss: 2.3849 - accuracy: 0.1204 - val_loss: 2.4107 - val_accuracy: 0.1260\n",
            "Epoch 4/40\n",
            "15/15 [==============================] - 3s 220ms/step - loss: 2.3513 - accuracy: 0.1291 - val_loss: 2.3720 - val_accuracy: 0.1102\n",
            "Epoch 5/40\n",
            "15/15 [==============================] - 3s 202ms/step - loss: 2.2943 - accuracy: 0.1904 - val_loss: 2.3269 - val_accuracy: 0.1496\n",
            "Epoch 6/40\n",
            "15/15 [==============================] - 3s 202ms/step - loss: 2.2350 - accuracy: 0.1904 - val_loss: 2.2773 - val_accuracy: 0.2126\n",
            "Epoch 7/40\n",
            "15/15 [==============================] - 3s 207ms/step - loss: 2.1567 - accuracy: 0.2473 - val_loss: 2.2200 - val_accuracy: 0.2756\n",
            "Epoch 8/40\n",
            "15/15 [==============================] - 3s 203ms/step - loss: 2.0615 - accuracy: 0.3042 - val_loss: 2.1415 - val_accuracy: 0.2441\n",
            "Epoch 9/40\n",
            "15/15 [==============================] - 3s 203ms/step - loss: 1.9477 - accuracy: 0.3414 - val_loss: 2.0956 - val_accuracy: 0.2913\n",
            "Epoch 10/40\n",
            "15/15 [==============================] - 3s 204ms/step - loss: 1.7619 - accuracy: 0.4311 - val_loss: 1.9724 - val_accuracy: 0.3543\n",
            "Epoch 11/40\n",
            "15/15 [==============================] - 3s 204ms/step - loss: 1.6332 - accuracy: 0.4639 - val_loss: 1.8579 - val_accuracy: 0.4331\n",
            "Epoch 12/40\n",
            "15/15 [==============================] - 3s 206ms/step - loss: 1.4546 - accuracy: 0.5339 - val_loss: 1.7288 - val_accuracy: 0.4646\n",
            "Epoch 13/40\n",
            "15/15 [==============================] - 3s 206ms/step - loss: 1.2394 - accuracy: 0.6346 - val_loss: 1.6017 - val_accuracy: 0.5197\n",
            "Epoch 14/40\n",
            "15/15 [==============================] - 3s 206ms/step - loss: 1.0503 - accuracy: 0.7090 - val_loss: 1.8479 - val_accuracy: 0.4252\n",
            "Epoch 15/40\n",
            "15/15 [==============================] - 3s 204ms/step - loss: 0.9289 - accuracy: 0.7265 - val_loss: 1.5407 - val_accuracy: 0.4803\n",
            "Epoch 16/40\n",
            "15/15 [==============================] - 3s 206ms/step - loss: 0.7532 - accuracy: 0.7899 - val_loss: 1.7316 - val_accuracy: 0.5276\n",
            "Epoch 17/40\n",
            "15/15 [==============================] - 3s 206ms/step - loss: 0.6642 - accuracy: 0.8074 - val_loss: 1.4093 - val_accuracy: 0.5433\n",
            "Epoch 18/40\n",
            "15/15 [==============================] - 6s 369ms/step - loss: 0.4809 - accuracy: 0.8972 - val_loss: 1.5815 - val_accuracy: 0.5591\n",
            "Epoch 19/40\n",
            "15/15 [==============================] - 5s 351ms/step - loss: 0.4150 - accuracy: 0.9147 - val_loss: 1.4449 - val_accuracy: 0.5827\n",
            "Epoch 20/40\n",
            "15/15 [==============================] - 3s 203ms/step - loss: 0.3305 - accuracy: 0.9344 - val_loss: 1.2851 - val_accuracy: 0.5748\n",
            "Epoch 21/40\n",
            "15/15 [==============================] - 3s 208ms/step - loss: 0.2620 - accuracy: 0.9497 - val_loss: 1.3182 - val_accuracy: 0.5669\n",
            "Epoch 22/40\n",
            "15/15 [==============================] - 3s 203ms/step - loss: 0.2205 - accuracy: 0.9628 - val_loss: 1.4866 - val_accuracy: 0.5906\n",
            "Epoch 23/40\n",
            "15/15 [==============================] - 3s 203ms/step - loss: 0.2493 - accuracy: 0.9453 - val_loss: 1.9566 - val_accuracy: 0.5118\n",
            "Epoch 24/40\n",
            "15/15 [==============================] - 3s 205ms/step - loss: 0.1780 - accuracy: 0.9562 - val_loss: 1.2602 - val_accuracy: 0.5827\n",
            "Epoch 25/40\n",
            "15/15 [==============================] - 3s 205ms/step - loss: 0.1442 - accuracy: 0.9672 - val_loss: 1.3329 - val_accuracy: 0.6299\n",
            "Epoch 26/40\n",
            "15/15 [==============================] - 3s 204ms/step - loss: 0.1073 - accuracy: 0.9825 - val_loss: 1.4543 - val_accuracy: 0.5984\n",
            "Epoch 27/40\n",
            "15/15 [==============================] - 3s 206ms/step - loss: 0.1020 - accuracy: 0.9869 - val_loss: 1.6595 - val_accuracy: 0.5906\n",
            "Epoch 28/40\n",
            "15/15 [==============================] - 3s 203ms/step - loss: 0.1197 - accuracy: 0.9847 - val_loss: 1.4259 - val_accuracy: 0.5827\n",
            "Epoch 29/40\n",
            "15/15 [==============================] - 3s 204ms/step - loss: 0.0885 - accuracy: 0.9869 - val_loss: 1.4746 - val_accuracy: 0.6220\n",
            "Epoch 30/40\n",
            "15/15 [==============================] - 3s 205ms/step - loss: 0.0584 - accuracy: 0.9978 - val_loss: 1.3127 - val_accuracy: 0.6378\n",
            "Epoch 31/40\n",
            "15/15 [==============================] - 3s 207ms/step - loss: 0.0568 - accuracy: 0.9956 - val_loss: 1.5164 - val_accuracy: 0.6535\n",
            "Epoch 32/40\n",
            "15/15 [==============================] - 3s 208ms/step - loss: 0.0586 - accuracy: 0.9869 - val_loss: 1.4069 - val_accuracy: 0.6220\n",
            "Epoch 33/40\n",
            "15/15 [==============================] - 3s 210ms/step - loss: 0.0553 - accuracy: 0.9912 - val_loss: 1.5296 - val_accuracy: 0.6220\n",
            "Epoch 34/40\n",
            "15/15 [==============================] - 3s 204ms/step - loss: 0.0546 - accuracy: 0.9912 - val_loss: 1.5135 - val_accuracy: 0.6299\n",
            "Epoch 35/40\n",
            "15/15 [==============================] - 3s 203ms/step - loss: 0.0414 - accuracy: 0.9978 - val_loss: 1.4856 - val_accuracy: 0.6299\n",
            "Epoch 36/40\n",
            "15/15 [==============================] - 3s 203ms/step - loss: 0.0440 - accuracy: 0.9934 - val_loss: 1.4545 - val_accuracy: 0.6457\n",
            "Epoch 37/40\n",
            "15/15 [==============================] - 3s 205ms/step - loss: 0.0388 - accuracy: 0.9934 - val_loss: 1.5040 - val_accuracy: 0.6378\n",
            "Epoch 38/40\n",
            "15/15 [==============================] - 3s 204ms/step - loss: 0.0380 - accuracy: 1.0000 - val_loss: 1.5177 - val_accuracy: 0.6220\n",
            "Epoch 39/40\n",
            "15/15 [==============================] - 3s 206ms/step - loss: 0.0396 - accuracy: 0.9956 - val_loss: 1.4700 - val_accuracy: 0.6457\n",
            "Epoch 40/40\n",
            "15/15 [==============================] - 3s 204ms/step - loss: 0.0295 - accuracy: 0.9978 - val_loss: 1.5696 - val_accuracy: 0.6378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0HJ4nwTqwsY"
      },
      "source": [
        "### with glove embedding\n",
        "not working lol"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1SCEs3yGHDz",
        "outputId": "c2e68094-2548-463d-b8fb-b7d0b83dbef0"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-23 20:01:16--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-11-23 20:01:16--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-11-23 20:01:16--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.11MB/s    in 6m 28s  \n",
            "\n",
            "2020-11-23 20:07:44 (2.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89OhhPfLuIY6",
        "outputId": "b3e3911f-6934-4c22-ce26-a347e0b89e9b"
      },
      "source": [
        "! ls"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive\t\t   glove.6B.200d.txt  glove.6B.50d.txt\tsample_data\n",
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob59wttXsjVe"
      },
      "source": [
        "import os\n",
        "import pathlib\n"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgLFrWKouVMm",
        "outputId": "fdffd8f6-5a08-44b8-e58f-a6b4fdebacbe"
      },
      "source": [
        "embeddings_index = {}\n",
        "with open('/content/glove.6B.100d.txt') as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQs9FOfouYYS"
      },
      "source": [
        "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    \n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "id5Hp8LhutIb"
      },
      "source": [
        "glove_embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            100,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=max_length,\n",
        "                            trainable=False)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRoijeRZyZo_",
        "outputId": "b502232c-5f36-4b6c-a2d0-f6f36c302c1b"
      },
      "source": [
        ""
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'embedding_25/embedding_lookup_1/Identity_1:0' shape=(None, 20, 300) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLaIJ5q74cdY"
      },
      "source": [
        "from keras.layers import Flatten"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnlzfHwkxo8m",
        "outputId": "b8eff0c7-6594-41bb-83f3-5c316792911a"
      },
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "nclasses = 12\n",
        "\n",
        "inputs = layers.Input(shape=(max_length,))\n",
        "#print(inputs.shape)\n",
        "embedding_layer = TokenAndPositionEmbedding(max_length, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "print(x.shape)\n",
        "x = glove_embedding_layer(x)\n",
        "print(x.shape)\n",
        "#x = Flatten(x)\n",
        "#print(x.shape)\n",
        "#transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "transformer_block = TransformerBlock(100, num_heads, 100)\n",
        "x = transformer_block(x)\n",
        "print(x.shape)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(nclasses, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 300, 32)\n",
            "(None, 300, 32, 100)\n",
            "(None, 300, 32, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhb19mcAxyRF"
      },
      "source": [
        "model.compile('adam', \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nOI0Ht1x16x",
        "outputId": "4f8e999f-3786-4966-8500-ef15835ea0cd"
      },
      "source": [
        "history = model.fit(\n",
        "    X_train_padded, y_train, batch_size=32, epochs=40, validation_data=(X_val_padded, y_val)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['token_and_position_embedding_21/embedding_45/embeddings:0', 'token_and_position_embedding_21/embedding_46/embeddings:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['token_and_position_embedding_21/embedding_45/embeddings:0', 'token_and_position_embedding_21/embedding_46/embeddings:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['token_and_position_embedding_21/embedding_45/embeddings:0', 'token_and_position_embedding_21/embedding_46/embeddings:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['token_and_position_embedding_21/embedding_45/embeddings:0', 'token_and_position_embedding_21/embedding_46/embeddings:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QGXLU8wx4Et"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}