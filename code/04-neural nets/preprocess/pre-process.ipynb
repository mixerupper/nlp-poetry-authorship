{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdata\u001b[m\u001b[m                  \u001b[34mnlp-poetry-authorship\u001b[m\u001b[m pre-process.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/christineshen/Desktop/NYU/Fall 2020/NLP/projects/data/train_val_test_split\n"
     ]
    }
   ],
   "source": [
    "cd ./data/train_val_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data.csv  train_data.csv val_data.csv\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_data.csv')\n",
    "val_df = pd.read_csv('val_data.csv')\n",
    "test_df = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>poetry_foundation_id</th>\n",
       "      <th>content</th>\n",
       "      <th>author_poem_count</th>\n",
       "      <th>author_poem_index</th>\n",
       "      <th>author_poem_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Percy sshe Shelley</td>\n",
       "      <td>from\\n  \\n  Queen Mab: Part VI</td>\n",
       "      <td>45137</td>\n",
       "      <td>(excerpt)\\n\"Throughout these infinite orbs of ...</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thomas Hardy</td>\n",
       "      <td>'According to the Mighty Working'</td>\n",
       "      <td>57342</td>\n",
       "      <td>I\\n\\nWhen moiling seems at cease\\nIn the vague...</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rae Armantrout</td>\n",
       "      <td>Our Nature</td>\n",
       "      <td>54881</td>\n",
       "      <td>The very flatness\\nof portraits\\nmakes for nos...</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Walt Whitman</td>\n",
       "      <td>For You O Democracy</td>\n",
       "      <td>51567</td>\n",
       "      <td>Come, I will make the continent indissoluble,\\...</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>William Butler Yeats</td>\n",
       "      <td>The Magi</td>\n",
       "      <td>12892</td>\n",
       "      <td>Now as at all times I can see in the mind's ey...</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                              title  \\\n",
       "0    Percy sshe Shelley     from\\n  \\n  Queen Mab: Part VI   \n",
       "1          Thomas Hardy  'According to the Mighty Working'   \n",
       "2        Rae Armantrout                         Our Nature   \n",
       "3          Walt Whitman                For You O Democracy   \n",
       "4  William Butler Yeats                           The Magi   \n",
       "\n",
       "   poetry_foundation_id                                            content  \\\n",
       "0                 45137  (excerpt)\\n\"Throughout these infinite orbs of ...   \n",
       "1                 57342  I\\n\\nWhen moiling seems at cease\\nIn the vague...   \n",
       "2                 54881  The very flatness\\nof portraits\\nmakes for nos...   \n",
       "3                 51567  Come, I will make the continent indissoluble,\\...   \n",
       "4                 12892  Now as at all times I can see in the mind's ey...   \n",
       "\n",
       "   author_poem_count  author_poem_index  author_poem_pct  \n",
       "0                 43                  0              0.0  \n",
       "1                 38                  0              0.0  \n",
       "2                 62                  0              0.0  \n",
       "3                 41                  0              0.0  \n",
       "4                 47                  0              0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.author.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I\\n\\nWhen moiling seems at cease\\nIn the vague void of night-time,\\nAnd heaven's wide roomage stormless\\nBetween the dusk and light-time,\\nAnd fear at last is formless,\\nWe call the allurement Peace.\\n\\nII\\n\\nPeace, this hid riot, Change,\\nThis revel of quick-cued mumming,\\nThis never truly being,\\nThis evermore becoming,\\nThis spinner's wheel onfleeing\\nOutside perception's range.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.content[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-3.5.1-py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /anaconda3/lib/python3.7/site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: packaging in /anaconda3/lib/python3.7/site-packages (from transformers) (20.3)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-3.14.0-cp37-cp37m-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /anaconda3/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /anaconda3/lib/python3.7/site-packages (from transformers) (4.43.0)\n",
      "Requirement already satisfied: sacremoses in /anaconda3/lib/python3.7/site-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.7/site-packages (from transformers) (1.18.1)\n",
      "Collecting tokenizers==0.9.3\n",
      "  Downloading tokenizers-0.9.3-cp37-cp37m-macosx_10_11_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 9.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /anaconda3/lib/python3.7/site-packages (from transformers) (2020.2.18)\n",
      "Collecting sentencepiece==0.1.91\n",
      "  Downloading sentencepiece-0.1.91-cp37-cp37m-macosx_10_6_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 17.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /anaconda3/lib/python3.7/site-packages (from packaging->transformers) (2.4.6)\n",
      "Requirement already satisfied: click in /anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied: joblib in /anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Installing collected packages: protobuf, tokenizers, sentencepiece, transformers\n",
      "Successfully installed protobuf-3.14.0 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = train_df.content[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I\\n\\nWhen moiling seems at cease\\nIn the vague void of night-time,\\nAnd heaven's wide roomage stormless\\nBetween the dusk and light-time,\\nAnd fear at last is formless,\\nWe call the allurement Peace.\\n\\nII\\n\\nPeace, this hid riot, Change,\\nThis revel of quick-cued mumming,\\nThis never truly being,\\nThis evermore becoming,\\nThis spinner's wheel onfleeing\\nOutside perception's range.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 146, 1332, 182, 20708, 1158, 3093, 1120, 14092, 1130, 1103, 14673, 13340, 1104, 1480, 118, 1159, 117, 1262, 9775, 112, 188, 2043, 1395, 2553, 4162, 2008, 3847, 1103, 24494, 1105, 1609, 118, 1159, 117, 1262, 2945, 1120, 1314, 1110, 1532, 2008, 117, 1284, 1840, 1103, 1155, 3313, 1880, 5370, 119, 1563, 5370, 117, 1142, 11269, 14807, 117, 9091, 117, 1188, 1231, 12559, 1104, 3613, 118, 18780, 1181, 23993, 5031, 117, 1188, 1309, 5098, 1217, 117, 1188, 1518, 4982, 2479, 117, 1188, 6898, 2511, 112, 188, 4829, 1113, 23445, 20309, 1403, 9572, 11170, 112, 188, 2079, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check out the how the tokenizer work on the example \n",
    "tokenizer(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import torch.nn as nn\n",
    "# from torch import optim\n",
    "# import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import os\n",
    "import time\n",
    "from tqdm import notebook\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ref: https://medium.com/analytics-vidhya/text-classification-with-bert-using-transformers-for-long-text-inputs-f54833994dfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def get_split(text1):\n",
    "    l_total = []\n",
    "    l_partial = []\n",
    "    if len(text1.split())//150 > 0:\n",
    "        n = len(text1.split())//150\n",
    "    else:\n",
    "        n = 1\n",
    "    for w in range(n):\n",
    "        if w == 0:\n",
    "            l_partial = text1.split()[:200]\n",
    "            l_total.append(\" \".join(l_partial))\n",
    "        else:\n",
    "            l_partial = text1.split()[w*150:w*150 + 200]\n",
    "            l_total.append(\" \".join(l_partial))\n",
    "    return l_total \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, poems, targets, tokenizer, max_len):\n",
    "        self.poems = poems\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.poems)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        poem = str(self.poems[item])\n",
    "        target = self.targets[item]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            poem,\n",
    "            add_special_tokens = True,\n",
    "            max_length = self.max_len,\n",
    "            return_token_type_ids = False,\n",
    "            pad_to_max_length = True,\n",
    "            return_attention_mask = True,\n",
    "            return_tensors = 'pt',)\n",
    "        \n",
    "        output = {\n",
    "            'poem_text': poem,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype = torch.long)\n",
    "        }\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = ReviewDataset(\n",
    "            poems = df.content.to_numpy(),\n",
    "            targets = df.author.to_numpy(),\n",
    "            tokenizer = tokenizer,\n",
    "            max_len = max_len)\n",
    "    \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['source_len'] = train_df['content'].apply(lambda x: len(x.replace('\\n',' ').split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9632"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = int(train_df['source_len'].quantile(0.999)) ###\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = create_data_loader(train_df, tokenizer, MAX_LEN, batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1a203f0a20>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
